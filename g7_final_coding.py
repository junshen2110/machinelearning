# -*- coding: utf-8 -*-
"""G7_final_coding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mPDDJe_c-KNFiI0X4ru_WFhcb6Ef7Njo

# **Leveraging Unsupervised Clustering of Wind Patterns to Enhance Urban Sustainability in Smart Cities**

## Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import zscore
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.svm import OneClassSVM, SVC
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.model_selection import GridSearchCV
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import gdown
import itertools

"""## Load Dataset"""

file_id = "18GOX3qdZVnLqTEjAFMIP6O-1wKOJ0Rtj"
# Create a downloadable URL
download_url = f"https://drive.google.com/uc?id={file_id}"

# Download the file
output_path = "my_file.csv"
gdown.download(download_url, output_path, quiet=False)

# Read the CSV into a DataFrame
df = pd.read_csv(output_path)

"""## Explore Dataset"""

# Print the first five rows of the DataFrame.
print(df.head())

# Print the shape of the DataFrame.
print(df.shape)

# Print the column names of the DataFrame.
print(df.columns)

# Print the data types of each column.
print(df.dtypes)

# Print the summary statistics of the DataFrame.
print(df.describe())

"""## Data Preprocessing

### Feature Engineering
"""

# Clean column names by stripping spaces
df.columns = df.columns.str.strip()

#Timestamp processing
df['hpwren_timestamp'] = pd.to_datetime(df['hpwren_timestamp'])

# Extract useful fe from the timestamp
df['year'] = df['hpwren_timestamp'].dt.year
df['month'] = df['hpwren_timestamp'].dt.month
df['day'] = df['hpwren_timestamp'].dt.day
df['hour'] = df['hpwren_timestamp'].dt.hour
df['minute'] = df['hpwren_timestamp'].dt.minute
df['second'] = df['hpwren_timestamp'].dt.second
df['timestamp'] = pd.to_datetime(df['hpwren_timestamp']).dt.time
df.rename(columns={'hpwren_timestamp': 'full_timestamp'}, inplace=True)
df.head()

"""### Missing Values"""

# Check for missing values
missing_values = df.isnull().sum()

# Display cleaned column names, missing values
df.columns, missing_values

# Drop all rows with any missing values in the DataFrame
print("Before drop:", df.shape)
df = df.dropna()
print("After drop:", df.shape)

"""### Duplicates rows"""

# Check for duplicate rows
duplicated_rows = df.duplicated().sum()
print(f"Number of duplicated rows: {duplicated_rows}")

"""### Invalid Values"""

print(df['rain_accumulation'].value_counts())

df['rain_accumulation'] = df['rain_duration'].apply(lambda x: 1 if x != 0 else 0)
print(df['rain_accumulation'].value_counts())
print(df['rain_duration'].value_counts())

"""### Outliers"""

outlier_cols = [
    'air_pressure', 'air_temp',
    'avg_wind_direction', 'avg_wind_speed',
    'max_wind_direction', 'max_wind_speed',
    'min_wind_direction', 'min_wind_speed',
    'relative_humidity'
]
plt.figure(figsize=(12, 6))
df[[col for col in outlier_cols if col != 'rain_duration']].boxplot(rot=45)
plt.title("Boxplot of Potential Outlier Columns")
plt.show()

non0_rain = df.loc[df['rain_duration'] != 0, 'rain_duration']
plt.figure(figsize=(12, 6))
non0_rain.plot.box(rot=45)
plt.title("Boxplot of Rain Duration")
plt.show()

# 1. Outlier detection for general columns using IQR
Q1 = df[outlier_cols].quantile(0.25)
Q3 = df[outlier_cols].quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Count outliers for each column in outlier_cols
outliers_count = ((df[outlier_cols] < lower_bound) | (df[outlier_cols] > upper_bound)).sum()
print("Outliers count for general columns:\n", outliers_count)

# 2. Outlier detection for rain_duration using z-score (for nonzero values only)
non0_rain = df.loc[df['rain_duration'] != 0, 'rain_duration']
z_scores = stats.zscore(non0_rain)
threshold = 3
non0_outliers_mask = np.abs(z_scores) > threshold
rain_duration_z_outliers_count = non0_outliers_mask.sum()
print("Outlier count for rain_duration (nonzero values, using zscore):", rain_duration_z_outliers_count)

# Get the indices of nonzero rain_duration outliers
non0_outlier_indices = non0_rain.index[non0_outliers_mask]

"""### Remove Outliers"""

print("Initial DataFrame shape:", df.shape)
# Remove rows with outliers in general columns (using IQR)
df = df[~((df[outlier_cols] < lower_bound) | (df[outlier_cols] > upper_bound)).any(axis=1)]

# Remove rows where rain_duration is nonzero and flagged as an outlier based on z-score
df = df[~((df['rain_duration'] != 0) & (df.index.isin(non0_outlier_indices)))]

print("DataFrame shape after outlier removal:", df.shape)

"""### Wind Direction"""

# Convert wind direction columns into x, y components
for col in ['avg_wind_direction', 'min_wind_direction', 'max_wind_direction']:
    df[f'{col}_x'] = np.cos(np.radians(df[col]))
    df[f'{col}_y'] = np.sin(np.radians(df[col]))


# Print the transformed dataset
print(df.head())


# Recover wind direction angles from x, y
df['avg_wind_direction_deg'] = np.degrees(np.arctan2(df['avg_wind_direction_y'], df['avg_wind_direction_x'])) % 360
df['min_wind_direction_deg'] = np.degrees(np.arctan2(df['min_wind_direction_y'], df['min_wind_direction_x'])) % 360
df['max_wind_direction_deg'] = np.degrees(np.arctan2(df['max_wind_direction_y'], df['max_wind_direction_x'])) % 360


print(df[['avg_wind_direction_deg', 'min_wind_direction_deg', 'max_wind_direction_deg']].head())

"""### Data Scaling"""

# Columns to scale (excluding wind direction)
features_to_scale = ['air_pressure', 'air_temp', 'avg_wind_speed',
                     'max_wind_speed', 'min_wind_speed',
                     'rain_duration', 'relative_humidity']

# Apply MinMaxScaler
scaler = MinMaxScaler()
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])
df.describe()

"""## EDA

## **Visualize data**
"""

df.head()

#visualize = df.iloc[:, 1:-1]
visualize_columns = ['air_pressure', 'air_temp', 'avg_wind_direction', 'avg_wind_speed',
                     'max_wind_direction', 'max_wind_speed', 'min_wind_direction',
                     'min_wind_speed', 'relative_humidity', 'rain_accumulation', 'rain_duration']

visualize = df[visualize_columns]  # Select only these columns from df
visualize.describe()

"""#### Histogram"""

visualize.hist(figsize=(12, 8), bins=30)
plt.suptitle("Histograms of Features")
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Rain Accumulation Histogram
sns.histplot(df['rain_accumulation'], bins=50, ax=axes[0], color='blue')
axes[0].set_yscale('log')
axes[0].set_title("Rain Accumulation Distribution (Log Scale)")
axes[0].set_xlabel("Rain Accumulation")
axes[0].set_ylabel("Frequency")

# Rain Duration Histogram
sns.histplot(df['rain_duration'], bins=50, ax=axes[1], color='green')
axes[1].set_yscale('log')
axes[1].set_title("Rain Duration Distribution (Log Scale)")
axes[1].set_xlabel("Rain Duration")
axes[1].set_ylabel("Frequency")

plt.tight_layout()
plt.show()

sns.pairplot(visualize)
plt.show()

"""### Feature Correlation Heatmap"""

plt.figure(figsize=(10, 8))
sns.heatmap(visualize.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

"""### Wind Direction

"""

# Function to categorize wind direction into N, NE, E, SE, S, SW, W, NW
def categorize_wind_direction(degrees):
    bins = [0, 45, 90, 135, 180, 225, 270, 315, 360]  # Corrected bin edges
    labels = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']
    return pd.cut(degrees % 360, bins=bins, labels=labels, include_lowest=True)

# Apply categorization
df['avg_wind_category'] = categorize_wind_direction(df['avg_wind_direction'])
df['max_wind_category'] = categorize_wind_direction(df['max_wind_direction'])
df['min_wind_category'] = categorize_wind_direction(df['min_wind_direction'])

# Count occurrences
avg_counts = df['avg_wind_category'].value_counts().reindex(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'], fill_value=0)
max_counts = df['max_wind_category'].value_counts().reindex(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'], fill_value=0)
min_counts = df['min_wind_category'].value_counts().reindex(['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW'], fill_value=0)

# Create figure with 2 rows (3 bar charts on top, 3 polar plots below)
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Bar charts (Top Row)
axes[0, 0].bar(avg_counts.index, avg_counts.values, color='blue')
axes[0, 0].set_title("Average Wind Direction")
axes[0, 0].set_xlabel("Direction")
axes[0, 0].set_ylabel("Count")

axes[0, 1].bar(max_counts.index, max_counts.values, color='red')
axes[0, 1].set_title("Max Wind Direction")
axes[0, 1].set_xlabel("Direction")

axes[0, 2].bar(min_counts.index, min_counts.values, color='green')
axes[0, 2].set_title("Min Wind Direction")
axes[0, 2].set_xlabel("Direction")

# Polar plots (Bottom Row)
angles_avg = np.deg2rad(df['avg_wind_direction'])  # Convert to radians
angles_max = np.deg2rad(df['max_wind_direction'])
angles_min = np.deg2rad(df['min_wind_direction'])

# Average Wind Rose
ax1 = fig.add_subplot(2, 3, 4, projection='polar')
ax1.hist(angles_avg, bins=36, color='blue', alpha=0.75)
ax1.set_title("Wind Rose - Average Wind Direction")

# Max Wind Rose
ax2 = fig.add_subplot(2, 3, 5, projection='polar')
ax2.hist(angles_max, bins=36, color='red', alpha=0.75)
ax2.set_title("Wind Rose - Max Wind Direction")

# Min Wind Rose
ax3 = fig.add_subplot(2, 3, 6, projection='polar')
ax3.hist(angles_min, bins=36, color='green', alpha=0.75)
ax3.set_title("Wind Rose - Min Wind Direction")

# Adjust layout
plt.tight_layout()
plt.show()

plt.hist(df['avg_wind_direction_deg'], bins=36, edgecolor='black', alpha=0.7)  # 10° bins
plt.xlabel("Wind Direction (Degrees)")
plt.ylabel("Frequency")
plt.title("Wind Direction Distribution")
plt.show()

"""## **Modeling**

"""

features = [
    'air_pressure', 'air_temp', 'avg_wind_speed', 'max_wind_speed',
    'min_wind_speed', 'relative_humidity',
    'avg_wind_direction_x', 'avg_wind_direction_y',
    'min_wind_direction_x', 'min_wind_direction_y',
    'max_wind_direction_x', 'max_wind_direction_y'
]
df_sample = df[features].sample(n=10000, random_state=42)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_sample)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

def evaluate_model(X, labels, name):
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    if n_clusters < 2:
        return (name, n_clusters, -1.0, -1.0, -1.0)
    sil = silhouette_score(X, labels)
    db = davies_bouldin_score(X, labels)
    ch = calinski_harabasz_score(X, labels)
    return (name, n_clusters, sil, db, ch)

"""### Default Parameter

"""

# 1. Clustering - KMeans
kmeans = KMeans().fit(X_scaled)  # Default n_clusters=8, random_state=None
labels_kmeans = kmeans.labels_

# 2. Clustering - DBSCAN
dbscan = DBSCAN().fit(X_scaled)  # Default eps=0.5, min_samples=5
labels_dbscan = dbscan.labels_

# 3. Clustering - Hierarchical
hierarchical = AgglomerativeClustering().fit(X_scaled)  # Default n_clusters=2
labels_hierarchical = hierarchical.labels_

# 4. Clustering - Gaussian Mixture
gmm = GaussianMixture().fit(X_scaled)  # Default n_components=1, covariance_type='full', random_state=None
labels_gmm = gmm.predict(X_scaled)

# 5. Clustering - Support Vector Clustering (SVC)
svc = OneClassSVM().fit(X_scaled)  # Default kernel='rbf', nu=0.5
labels_svc = svc.predict(X_scaled)
labels_svc = [0 if l == -1 else 1 for l in labels_svc]

# 6. Clustering - Spectral Clustering
spectral = SpectralClustering().fit(X_scaled)  # Default n_clusters=8, affinity='nearest_neighbors', random_state=None
labels_spectral = spectral.labels_

results = [
    evaluate_model(X_scaled, labels_kmeans, "KMeans"),
    evaluate_model(X_scaled, labels_dbscan, "DBSCAN"),
    evaluate_model(X_scaled, labels_hierarchical, "Hierarchical"),
    evaluate_model(X_scaled, labels_gmm, "Gaussian Mixture"),
    evaluate_model(X_scaled, labels_spectral, "Spectral Clustering"),
    evaluate_model(X_scaled, labels_svc, "SVM Clustering")
]

results_df = pd.DataFrame(results, columns=["Model", "Num Clusters", "Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Index"])

# Sort by Silhouette Score (or choose another metric)
results_df.sort_values(by="Silhouette Score", ascending=False, inplace=True)

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. Silhouette Score
results_df.plot(x="Model", y="Silhouette Score", kind="bar", legend=False, color='skyblue', ax=axes[0])
axes[0].set_title("Silhouette Score by Model")
axes[0].set_ylabel("Silhouette Score")
axes[0].tick_params(axis='x', rotation=45)

# 2. Davies-Bouldin Index
results_df.plot(x="Model", y="Davies-Bouldin Index", kind="bar", legend=False, color='salmon', ax=axes[1])
axes[1].set_title("Davies-Bouldin Index (Lower is Better)")
axes[1].set_ylabel("Davies-Bouldin Index")
axes[1].tick_params(axis='x', rotation=45)

# 3. Calinski-Harabasz Index
results_df.plot(x="Model", y="Calinski-Harabasz Index", kind="bar", legend=False, color='seagreen', ax=axes[2])
axes[2].set_title("Calinski-Harabasz Index (Higher is Better)")
axes[2].set_ylabel("Calinski-Harabasz Index")
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
transposed_df = results_df.transpose()

# Print the transposed DataFrame
print(transposed_df)

# Visualize clusters using PCA (2 rows of 3 plots each)
fig, axs = plt.subplots(2, 3, figsize=(18, 10))  # 2 rows, 3 columns
axs[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_kmeans, cmap='tab10')
axs[0, 0].set_title('KMeans Clustering')

axs[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_dbscan, cmap='tab10')
axs[0, 1].set_title('DBSCAN Clustering')

axs[0, 2].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_hierarchical, cmap='tab10')
axs[0, 2].set_title('Hierarchical Clustering')

axs[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_gmm, cmap='tab10')
axs[1, 0].set_title('Gaussian Mixture')

axs[1, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_svc, cmap='tab10')
axs[1, 1].set_title('SVM Clustering')

axs[1, 2].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_spectral, cmap='tab10')
axs[1, 2].set_title('Spectral Clustering')

plt.tight_layout()
plt.show()

# Display the results dataframe
results_df

"""### HyperParameter Tuning

"""

k_range = range(2, 10)
kmeans_results = []
hierarchical_results = []

for k in k_range:
    # KMeans
    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)
    eval_km = evaluate_model(X_scaled, km.labels_, 'KMeans')
    kmeans_results.append((k, *eval_km[2:]))

    # Hierarchical
    hc = AgglomerativeClustering(n_clusters=k).fit(X_scaled)
    eval_hc = evaluate_model(X_scaled, hc.labels_, 'Hierarchical')
    hierarchical_results.append((k, *eval_hc[2:]))

# Visualize Dendrogram
linkage_matrix = linkage(X_scaled, method='average')
plt.figure(figsize=(10, 5))
dendrogram(linkage_matrix, truncate_mode='lastp', p=30, leaf_rotation=90., leaf_font_size=10.)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Sample Index or (Cluster Size)")
plt.ylabel("Distance")
plt.tight_layout()
plt.grid(True)
plt.show()

# Convert to DataFrames (keep these only)
kmeans_df = pd.DataFrame(kmeans_results, columns=["n_clusters", "Silhouette", "Davies-Bouldin", "Calinski-Harabasz"])
hierarchical_df = pd.DataFrame(hierarchical_results, columns=["n_clusters", "Silhouette", "Davies-Bouldin", "Calinski-Harabasz"])

# Sort and display top 10 KMeans configs
kmeans_df_sorted = kmeans_df.sort_values(by='Silhouette', ascending=False)
print(kmeans_df_sorted.head(10))

# Sort and display top 10 Hierarchical configs
hierarchical_df_sorted = hierarchical_df.sort_values(by='Silhouette', ascending=False)
print(hierarchical_df_sorted.head(10))

# Define color maps for both models
color_map_kmeans = 'b'  # Blue for KMeans
color_map_hierarchical = 'r'  # Red for Hierarchical

# Create subplots
fig, axs = plt.subplots(1, 3, figsize=(18, 5))

# Plot Silhouette Score comparison
axs[0].plot(kmeans_df['n_clusters'], kmeans_df['Silhouette'], marker='o', label='KMeans', color=color_map_kmeans)
axs[0].plot(hierarchical_df['n_clusters'], hierarchical_df['Silhouette'], marker='o', label='Hierarchical', color=color_map_hierarchical)
axs[0].set_title("Silhouette Score vs. n_clusters")
axs[0].set_xlabel('Number of Clusters')
axs[0].set_ylabel('Silhouette Score')
axs[0].legend()

# Plot Davies-Bouldin Index comparison
axs[1].plot(kmeans_df['n_clusters'], kmeans_df['Davies-Bouldin'], marker='o', label='KMeans', color=color_map_kmeans)
axs[1].plot(hierarchical_df['n_clusters'], hierarchical_df['Davies-Bouldin'], marker='o', label='Hierarchical', color=color_map_hierarchical)
axs[1].set_title("Davies-Bouldin Index vs. n_clusters")
axs[1].set_xlabel('Number of Clusters')
axs[1].set_ylabel('Davies-Bouldin Index')
axs[1].legend()

# Plot Calinski-Harabasz Index comparison
axs[2].plot(kmeans_df['n_clusters'], kmeans_df['Calinski-Harabasz'], marker='o', label='KMeans', color=color_map_kmeans)
axs[2].plot(hierarchical_df['n_clusters'], hierarchical_df['Calinski-Harabasz'], marker='o', label='Hierarchical', color=color_map_hierarchical)
axs[2].set_title("Calinski-Harabasz Index vs. n_clusters")
axs[2].set_xlabel('Number of Clusters')
axs[2].set_ylabel('Calinski-Harabasz Index')
axs[2].legend()

# Adjust layout
plt.tight_layout()
plt.show()

eps_values = np.linspace(0.2, 1.1, 10)
min_samples_values = [5, 10, 15]
dbscan_results = []

for eps in eps_values:
    for min_samples in min_samples_values:
        try:
            db = DBSCAN(eps=eps, min_samples=min_samples).fit(X_scaled)
            labels = db.labels_
            name, n_clusters, sil, db_score, ch = evaluate_model(X_scaled, labels, "DBSCAN")
            dbscan_results.append({
                'eps': eps,
                'min_samples': min_samples,
                'n_clusters': n_clusters,
                'Silhouette': sil,
                'Davies-Bouldin': db_score,
                'Calinski-Harabasz': ch
            })
        except Exception as e:
            print(f"Failed: eps={eps}, min_samples={min_samples} → {e}")

dbscan_df = pd.DataFrame(dbscan_results)
dbscan_sorted = dbscan_df.sort_values(by="Silhouette", ascending=False)

# Show top DBSCAN configurations
print("\nTop DBSCAN Configurations:")
print(dbscan_sorted.head(10))

# Create subplots for 3 different scores: Silhouette, Davies-Bouldin, and Calinski-Harabasz
fig, axs = plt.subplots(1, 3, figsize=(18, 5))

# Plot Silhouette Score comparison
for min_samples in min_samples_values:
    subset = dbscan_df[dbscan_df['min_samples'] == min_samples]
    axs[0].plot(subset['eps'], subset['Silhouette'], marker='o', label=f"min_samples={min_samples}")
axs[0].set_title("DBSCAN: Silhouette Score vs. eps")
axs[0].set_xlabel('eps')
axs[0].set_ylabel('Silhouette Score')
axs[0].legend()

# Plot Davies-Bouldin Index comparison
for min_samples in min_samples_values:
    subset = dbscan_df[dbscan_df['min_samples'] == min_samples]
    axs[1].plot(subset['eps'], subset['Davies-Bouldin'], marker='o', label=f"min_samples={min_samples}")
axs[1].set_title("DBSCAN: Davies-Bouldin Index vs. eps")
axs[1].set_xlabel('eps')
axs[1].set_ylabel('Davies-Bouldin Index')
axs[1].legend()

# Plot Calinski-Harabasz Index comparison
for min_samples in min_samples_values:
    subset = dbscan_df[dbscan_df['min_samples'] == min_samples]
    axs[2].plot(subset['eps'], subset['Calinski-Harabasz'], marker='o', label=f"min_samples={min_samples}")
axs[2].set_title("DBSCAN: Calinski-Harabasz Index vs. eps")
axs[2].set_xlabel('eps')
axs[2].set_ylabel('Calinski-Harabasz Index')
axs[2].legend()

# Adjust layout for better readability
plt.tight_layout()
plt.show()

# Define the parameter grid for Gaussian Mixture Model (GMM)
param_grid_gmm = {
    'n_components': [2, 3, 4, 5, 6],
    'covariance_type': ['full', 'tied', 'diag', 'spherical']
}

# Initialize Gaussian Mixture Model
gmm = GaussianMixture(random_state=42)

# Perform GridSearchCV
grid_search_gmm = GridSearchCV(gmm, param_grid_gmm, cv=5)
grid_search_gmm.fit(X_scaled)

# Store results
results = []

for n in param_grid_gmm['n_components']:
    for cov_type in param_grid_gmm['covariance_type']:
        gmm = GaussianMixture(n_components=n, covariance_type=cov_type, random_state=42)
        gmm.fit(X_scaled)
        labels = gmm.predict(X_scaled)
        # Calculate scores

        name, n_clusters, sil, db, ch = evaluate_model(X_scaled, labels, "GMM")

        results.append({
            'covariance_type': cov_type,
            'n_components': n,
            'n_clusters': n_clusters,
            'Silhouette': sil,
            'Davies-Bouldin': db,
            'Calinski-Harabasz': ch
        })

# Create DataFrame of results
gmm_df = pd.DataFrame(results)

# Optional: sort by composite score or any metric
gmm_df_sorted = gmm_df.sort_values(by='Silhouette', ascending=False)

# Display
print(gmm_df_sorted.head(10))

# Define colors for each model (covariance type)
color_map = {
    'full': 'b',
    'tied': 'r',
    'diag': 'g',
    'spherical': 'm'
}

# Create subplots
fig, axs = plt.subplots(1, 3, figsize=(18, 5))

# Plot Silhouette Score by model type
for model, group in gmm_df_sorted.groupby('covariance_type'):
    axs[0].plot(group['n_clusters'], group['Silhouette'], marker='o', label=model, color=color_map.get(model, 'gray'))
    axs[0].set_title("GMM: Silhouette Score vs. n_components")
    axs[0].set_xlabel('Number of Clusters')
    axs[0].set_ylabel('Silhouette Score')
    axs[0].legend()

# Plot Davies-Bouldin Index by model type
for model, group in gmm_df_sorted.groupby('covariance_type'):
    axs[1].plot(group['n_clusters'], group['Davies-Bouldin'], marker='o', label=model, color=color_map.get(model, 'gray'))
    axs[1].set_title("GMM: Davies-Bouldin Index vs. n_components")
    axs[1].set_xlabel('Number of Clusters')
    axs[1].set_ylabel('Davies-Bouldin Index')
    axs[1].legend()

# Plot Calinski-Harabasz Index by model type
for model, group in gmm_df_sorted.groupby('covariance_type'):
    axs[2].plot(group['n_clusters'], group['Calinski-Harabasz'], marker='o', label=model, color=color_map.get(model, 'gray'))
    axs[2].set_title("GMM: Calinski-Harabasz Index vs. n_components")
    axs[2].set_xlabel('Number of Clusters')
    axs[2].set_ylabel('Calinski-Harabasz Index')
    axs[2].legend()

# Adjust layout
plt.tight_layout()
plt.show()

param_grid_svc = {
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
    'nu': [0.01, 0.05, 0.1]
}

results = []

# Loop through combinations
for kernel in param_grid_svc['kernel']:
    for nu in param_grid_svc['nu']:
        model = OneClassSVM(kernel=kernel, nu=nu)
        model.fit(X_scaled)

        labels = model.predict(X_scaled)
        labels = [0 if l == -1 else 1 for l in labels]

        # Evaluate
        model_eval = evaluate_model(X_scaled, labels, kernel)
        model_name = model_eval[0]
        n_clusters = model_eval[1]
        sil = model_eval[2]
        db = model_eval[3]
        ch = model_eval[4]

        results.append((model_name, nu, n_clusters, sil, db, ch))

# Create DataFrame
svc_df = pd.DataFrame(
    results,
    columns=['Model', 'Nu', 'n_clusters', 'Silhouette', 'Davies-Bouldin', 'Calinski-Harabasz']
)
# svc_df = pd.DataFrame(
#     [(r[1], r[2], r[0], r[1]) for r in results],
#     columns=['n_clusters', 'Silhouette', 'Kernel', 'Nu']
# )

# Sort by Silhouette score
svc_sorted = svc_df.sort_values(by='Silhouette', ascending=False)

# Show top 10 results
print(svc_sorted.head(10))

color_map = {
    'linear': 'b',
    'poly': 'r',
    'rbf': 'g',
    'sigmoid': 'm'
}

# Create subplots
fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # 3 subplots for Silhouette, Davies-Bouldin, and Calinski-Harabasz

# Plot Silhouette Score by kernel type against 'nu'
for kernel, group in svc_df.groupby('Model'):
    axs[0].plot(group['Nu'], group['Silhouette'], marker='o', label=kernel, color=color_map.get(kernel, 'gray'))
axs[0].set_title("OneClassSVM: Silhouette Score vs. nu")
axs[0].set_xlabel('nu')
axs[0].set_ylabel('Silhouette Score')
axs[0].legend()

# Plot Davies-Bouldin Index by kernel type against 'nu'
for kernel, group in svc_df.groupby('Model'):
    axs[1].plot(group['Nu'], group['Davies-Bouldin'], marker='o', label=kernel, color=color_map.get(kernel, 'gray'))
axs[1].set_title("OneClassSVM: Davies-Bouldin Index vs. nu")
axs[1].set_xlabel('nu')
axs[1].set_ylabel('Davies-Bouldin Index')
axs[1].legend()

# Plot Calinski-Harabasz Index by kernel type against 'nu'
for kernel, group in svc_df.groupby('Model'):
    axs[2].plot(group['Nu'], group['Calinski-Harabasz'], marker='o', label=kernel, color=color_map.get(kernel, 'gray'))
axs[2].set_title("OneClassSVM: Calinski-Harabasz Index vs. nu")
axs[2].set_xlabel('nu')
axs[2].set_ylabel('Calinski-Harabasz Index')
axs[2].legend()

# Adjust layout
plt.tight_layout()
plt.show()

# Hyperparameter grids
n_clusters_list = [2, 3, 4, 5, 6]
affinities = ['nearest_neighbors', 'rbf']

# Store results
results = []

for n_clusters, affinity in itertools.product(n_clusters_list, affinities):
    try:
        model = SpectralClustering(n_clusters=n_clusters, affinity=affinity, random_state=42)
        labels = model.fit_predict(X_scaled)

        # Evaluate model
        name, n_clusters_used, sil, db, ch = evaluate_model(X_scaled, labels, "Spectral")

        # Append result
        results.append({
            'n_clusters': n_clusters,
            'affinity': affinity,
            'Silhouette': sil,
            'Davies-Bouldin': db,
            'Calinski-Harabasz': ch
        })
    except Exception as e:
        print(f"Failed for: clusters={n_clusters}, affinity={affinity} -> {e}")

# Create DataFrame
spec_df = pd.DataFrame(results)

# Sort by Silhouette Score
best_results = spec_df.sort_values(by='Silhouette', ascending=False)
print(best_results.head(10))

# Color map for affinities
color_map = {
    'nearest_neighbors': 'b',
    'rbf': 'g'
}

# Create subplots
fig, axs = plt.subplots(1, 3, figsize=(18, 5))  # 3 subplots for Silhouette, Davies-Bouldin, and Calinski-Harabasz

# Plot Silhouette Score by affinity type against 'n_clusters'
for affinity, group in spec_df.groupby('affinity'):
    axs[0].plot(group['n_clusters'], group['Silhouette'], marker='o', label=affinity, color=color_map.get(affinity, 'gray'))
axs[0].set_title("SpectralClustering: Silhouette Score vs. n_clusters")
axs[0].set_xlabel('n_clusters')
axs[0].set_ylabel('Silhouette Score')
axs[0].legend()

# Plot Davies-Bouldin Index by affinity type against 'n_clusters'
for affinity, group in spec_df.groupby('affinity'):
    axs[1].plot(group['n_clusters'], group['Davies-Bouldin'], marker='o', label=affinity, color=color_map.get(affinity, 'gray'))
axs[1].set_title("SpectralClustering: Davies-Bouldin Index vs. n_clusters")
axs[1].set_xlabel('n_clusters')
axs[1].set_ylabel('Davies-Bouldin Index')
axs[1].legend()

# Plot Calinski-Harabasz Index by affinity type against 'n_clusters'
for affinity, group in spec_df.groupby('affinity'):
    axs[2].plot(group['n_clusters'], group['Calinski-Harabasz'], marker='o', label=affinity, color=color_map.get(affinity, 'gray'))
axs[2].set_title("SpectralClustering: Calinski-Harabasz Index vs. n_clusters")
axs[2].set_xlabel('n_clusters')
axs[2].set_ylabel('Calinski-Harabasz Index')
axs[2].legend()

# Adjust layout
plt.tight_layout()
plt.show()

"""### Best Parameter


"""

# Clustering - KMeans
kmeans = KMeans(n_clusters=2, random_state=42).fit(X_scaled)
labels_bestkmeans = kmeans.labels_

# Clustering - DBSCAN
dbscan = DBSCAN(eps=1.1, min_samples=10).fit(X_scaled)
labels_bestdbscan = dbscan.labels_

# Clustering - Hierarchical
hierarchical = AgglomerativeClustering(n_clusters=2).fit(X_scaled)
labels_besthierarchical = hierarchical.labels_

# 4. Clustering - Gaussian Mixture
gmm = GaussianMixture(n_components=2, covariance_type='spherical', random_state=42).fit(X_scaled)
labels_bestgmm = gmm.predict(X_scaled)

# 5. Clustering - Support Vector Clustering (SVC)
svc = OneClassSVM(kernel='rbf', nu=0.01).fit(X_scaled)
labels_bestsvc = svc.predict(X_scaled)
labels_bestsvc = [0 if l == -1 else 1 for l in labels_bestsvc]

# 6. Clustering - Spectral Clustering
spectral = SpectralClustering(n_clusters=2, random_state=42, affinity='nearest_neighbors').fit(X_scaled)
labels_bestspectral = spectral.labels_

best_results = [
    evaluate_model(X_scaled, labels_bestkmeans, "KMeans"),
    evaluate_model(X_scaled, labels_bestdbscan, "DBSCAN"),
    evaluate_model(X_scaled, labels_besthierarchical, "Hierarchical"),
    evaluate_model(X_scaled, labels_bestgmm, "Gaussian Mixture"),
    evaluate_model(X_scaled, labels_bestspectral, "Spectral Clustering"),
    evaluate_model(X_scaled, labels_bestsvc, "SVM Clustering")
]

best_results_df = pd.DataFrame(best_results, columns=["Model", "Num Clusters", "Silhouette Score", "Davies-Bouldin Index", "Calinski-Harabasz Index"])

# Sort by Silhouette Score (or choose another metric)
best_results_df.sort_values(by="Silhouette Score", ascending=False, inplace=True)

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# 1. Silhouette Score
best_results_df.plot(x="Model", y="Silhouette Score", kind="bar", legend=False, color='skyblue', ax=axes[0])
axes[0].set_title("Silhouette Score by Model")
axes[0].set_ylabel("Silhouette Score")
axes[0].tick_params(axis='x', rotation=45)

# 2. Davies-Bouldin Index
best_results_df.plot(x="Model", y="Davies-Bouldin Index", kind="bar", legend=False, color='salmon', ax=axes[1])
axes[1].set_title("Davies-Bouldin Index (Lower is Better)")
axes[1].set_ylabel("Davies-Bouldin Index")
axes[1].tick_params(axis='x', rotation=45)

# 3. Calinski-Harabasz Index
best_results_df.plot(x="Model", y="Calinski-Harabasz Index", kind="bar", legend=False, color='seagreen', ax=axes[2])
axes[2].set_title("Calinski-Harabasz Index (Higher is Better)")
axes[2].set_ylabel("Calinski-Harabasz Index")
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()
best_results_df

# Visualize clusters using PCA (2 rows of 3 plots each)
fig, axs = plt.subplots(2, 3, figsize=(18, 10))  # 2 rows, 3 columns
axs[0, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_bestkmeans, cmap='tab10')
axs[0, 0].set_title('KMeans Clustering')

axs[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_bestdbscan, cmap='tab10')
axs[0, 1].set_title('DBSCAN Clustering')

axs[0, 2].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_besthierarchical, cmap='tab10')
axs[0, 2].set_title('Hierarchical Clustering')

axs[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_bestgmm, cmap='tab10')
axs[1, 0].set_title('Gaussian Mixture')

axs[1, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_bestsvc, cmap='tab10')
axs[1, 1].set_title('SVM Clustering')

axs[1, 2].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_bestspectral, cmap='tab10')
axs[1, 2].set_title('Spectral Clustering')

plt.tight_layout()
plt.show()

# Display the results dataframe
best_results_df

# Interpret the Clusters
kmeans_centers = kmeans.cluster_centers_
kmeans_cluster_analysis = pd.DataFrame(kmeans_centers, columns=features)
print("\nKMeans Cluster Centers Analysis:")
print(kmeans_cluster_analysis)

"""Based on the KMeans Cluster Centers Analysis, let's qualitatively describe each cluster:

###**Cluster 0:**
*   Air Pressure: High air pressure (+0.34).
*   Temperature: Slightly higher than average (+0.14).
*   Wind Speed: Generally low average wind speed (-0.32) and low maximum wind speed (-0.30).
*   Rain Duration: Low or short periods of rain (-0.04).
*   Relative Humidity: Low relative humidity (-0.38), indicating drier conditions.
*   Wind Direction: Strong positive component in the x-axis of the wind direction (+1.01), meaning winds primarily come from the east or southeast.
*   Description: This cluster could represent steady, mild winds in areas with stable weather conditions and low moisture. The wind likely flows in a predominantly consistent direction but with low speeds, ideal for areas where ventilation or cooling is needed but not excessive force.
*   Qualitative description: "Moderate, steady winds with lower variability".

###**Cluster 1:**
*   Air Pressure: Lower air pressure (-0.26).
*   Temperature: Slightly cooler than average (-0.11).
*   Wind Speed: Moderate wind speeds (+0.24 for average and +0.23 for max wind speeds), indicating stronger winds compared to Cluster 0.
*   Rain Duration: Longer rain durations (+0.03), suggesting this cluster experiences more rainfall or longer rainfall periods.
*   Relative Humidity: Higher relative humidity (+0.29), indicating slightly more moisture in the air.
*   Wind Direction: Strong negative components in the x-axis of wind direction (-0.76), suggesting winds primarily come from the west or northwest.
*   Description: This cluster is associated with stronger, more variable winds, likely from changing weather patterns or different wind zones. The winds are more unpredictable, likely in areas affected by changing or turbulent weather conditions, with occasional rainstorms. This would be more suitable for regions needing wind energy generation or increased natural ventilation in buildings.
*   Qualitative description: "Stronger, more erratic winds".

###**Summary**
* Cluster 0 represents areas with low wind speed and dry, stable conditions with moderate airflow from the southeast.

* Cluster 1 represents regions with stronger winds and more humid, rainy conditions, likely with winds from the northwest.

* These clusters can provide insights into how urban design could be adjusted to improve ventilation or mitigate heat islands, depending on which type of wind pattern dominates different city areas.

# STREAMLIT
"""

!pip install streamlit
import streamlit as st

st.title('Leveraging Unsupervised Clustering of Wind Patterns to Enhance Urban Sustainability in Smart Cities')

streamlit run